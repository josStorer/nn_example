1
00:00:00,266 --> 00:00:03,099
前段时间AI绘画不断在我的动态出现

2
00:00:03,100 --> 00:00:04,866
 也屡次登上了B站热门

3
00:00:05,133 --> 00:00:07,766
 有人认为AI会帮助人类提高效率

4
00:00:07,900 --> 00:00:09,866
 有人认为AI是抄袭剽窃

5
00:00:10,166 --> 00:00:12,533
 也有人认为AI会取代各行各业

6
00:00:13,033 --> 00:00:14,866
AI究竟是如何工作的

7
00:00:15,066 --> 00:00:16,166
 为了解释这个问题

8
00:00:16,166 --> 00:00:18,199
 我在没有使用第三方框架的情况下

9
00:00:18,200 --> 00:00:21,700
 编写并开源了一个约100行代码的简易神经网络示例

10
00:00:21,700 --> 00:00:23,733
 来一起看看, AI的内部原理

11
00:00:24,100 --> 00:00:27,833
我会先用尽量零基础无门槛的方式讲解神经网络的原理

12
00:00:28,166 --> 00:00:31,066
然后对ai作画原理构成一个大致印象

13
00:00:31,866 --> 00:00:34,866
首先我需要先重新介绍一下数学的函数

14
00:00:35,233 --> 00:00:36,733
 也许有些冗长无趣

15
00:00:36,933 --> 00:00:38,666
 观众可以加速或跳过

16
00:00:38,733 --> 00:00:40,733
 结合分P标题跳转会更好

17
00:00:42,433 --> 00:00:44,699
y=x+1 是一个非常简单的函数

18
00:00:44,900 --> 00:00:46,966
 形状是一条斜率为1的直线

19
00:00:47,700 --> 00:00:49,966
其中x是输入, y是输出

20
00:00:50,633 --> 00:00:52,066
 给定任意x的值

21
00:00:52,066 --> 00:00:53,666
 y都会在此基础上加一

22
00:00:54,466 --> 00:00:56,433
函数也常常写成如f(x)=x+1

23
00:00:58,100 --> 00:00:59,333
括号内是参数

24
00:00:59,533 --> 00:01:01,933
 f可以指代对输入的参数的作用

25
00:01:02,433 --> 00:01:04,433
 带入x得到一个输出结果

26
00:01:04,866 --> 00:01:07,666
我们在学校也许常常会碰到类似这样的问题

27
00:01:08,166 --> 00:01:09,566
 一个水箱正在放水

28
00:01:10,066 --> 00:01:11,533
 水的总量是100升

29
00:01:11,900 --> 00:01:13,566
 放水速度是每秒4升

30
00:01:13,766 --> 00:01:15,566
 问x秒后剩余多少水

31
00:01:16,533 --> 00:01:19,599
于是我们列出 y = 100 - 4*x (0<=x<=25),

32
00:01:19,600 --> 00:01:21,533
其中x是消耗的时间

33
00:01:21,933 --> 00:01:24,533
 而y则是对剩余水量的一个"预测"输出

34
00:01:24,766 --> 00:01:27,566
我们可以赋予函数的输入输出参数实际意义

35
00:01:27,566 --> 00:01:29,166
 从而解决现实问题

36
00:01:29,333 --> 00:01:31,799
一个复杂的问题也许有许多参数

37
00:01:31,800 --> 00:01:39,466
 我们可以写成如 f(x1,x2,x3) = x1 + 3*x2 + 4*x3^2这样的形式

38
00:01:39,733 --> 00:01:41,566
不论是文字, 图像, 音频

39
00:01:41,900 --> 00:01:44,033
 这些都可以被转化为数据存储

40
00:01:44,200 --> 00:01:46,266
 所有的数据都可以作为函数的参数

41
00:01:46,266 --> 00:01:47,299
 得到一个输出

42
00:01:47,533 --> 00:01:50,233
例如我们想要做一个简单的数字判别函数

43
00:01:50,600 --> 00:01:52,266
 传入一个三乘三的矩阵

44
00:01:52,533 --> 00:01:54,433
 你也可以把它看作是一个图像

45
00:01:54,966 --> 00:01:57,433
 假定其形状一定是一个有效的数字

46
00:01:57,566 --> 00:02:00,066
 要求我们判断形状是否是数字1

47
00:02:00,500 --> 00:02:01,833
因为只有九个格子

48
00:02:01,933 --> 00:02:04,766
 能容纳的数字只有0,1,4,7

49
00:02:05,333 --> 00:02:07,333
我们注意到, 这个矩阵中

50
00:02:07,333 --> 00:02:09,133
 0,4只有一种写法

51
00:02:09,266 --> 00:02:12,199
 7有7种写法, 而1有9种写法

52
00:02:12,333 --> 00:02:14,599
我们当然可以针对这些写法一一判断

53
00:02:14,666 --> 00:02:17,133
 仅判断1只需要检查这九种情况

54
00:02:17,800 --> 00:02:19,400
 但是这里存在着规律

55
00:02:19,666 --> 00:02:20,466
例如

56
00:02:20,466 --> 00:02:23,233
 1的形状永远只占两个或三个像素

57
00:02:23,700 --> 00:02:24,800
 针对这个条件

58
00:02:24,800 --> 00:02:26,866
 我们可以筛选掉许多其他数字

59
00:02:27,200 --> 00:02:29,333
 只剩下7的四种写法与我们竞争

60
00:02:30,100 --> 00:02:33,100
并且7的写法, 一定会涉及中间一列

61
00:02:33,300 --> 00:02:35,066
 而1占三个像素时

62
00:02:35,066 --> 00:02:37,033
 只有一种写法占用了中间一列

63
00:02:37,033 --> 00:02:37,899
 且占满

64
00:02:38,600 --> 00:02:45,033
我们可以先统计总的占用像素量, y1=x1+x2+x3...x9

65
00:02:45,533 --> 00:02:47,299
这里隐藏了一个系数矩阵

66
00:02:48,333 --> 00:02:49,766
我们可以再构造一个矩阵

67
00:02:50,133 --> 00:02:51,666
 检测对中间一列的占用

68
00:02:52,466 --> 00:02:56,399
写成函数就是y2=x2+x5+x8

69
00:02:57,033 --> 00:02:58,566
将这两种条件结合起来

70
00:02:58,566 --> 00:03:00,299
 也就是y1(总数)=2

71
00:03:00,300 --> 00:03:03,066
或y1=3且y2(中间列)=3或0时

72
00:03:03,233 --> 00:03:04,533
 代表形状是数字1

73
00:03:05,133 --> 00:03:05,933
针对这个描述

74
00:03:05,933 --> 00:03:07,599
 我们仍然可以列出一个函数

75
00:03:07,666 --> 00:03:10,433
 如果z=0, 那么代表符合条件

76
00:03:10,666 --> 00:03:12,366
 形状是数字1

77
00:03:12,366 --> 00:03:14,266
z的三维图形是这样的

78
00:03:14,600 --> 00:03:17,166
 可以看到这个函数在作用域内

79
00:03:17,233 --> 00:03:19,399
 与z=0平面有一些交点

80
00:03:19,400 --> 00:03:21,533
 即是刚才提到的y1,y2的条件

81
00:03:21,766 --> 00:03:23,966
 其中y1=2的两个交点的连线

82
00:03:24,133 --> 00:03:25,299
 全部符合条件

83
00:03:25,466 --> 00:03:29,366
 即y1=2, 或y1=3且y2=3或0

84
00:03:29,666 --> 00:03:31,366
我们还可以将y1,y2带入

85
00:03:31,366 --> 00:03:35,399
 最终得到一个复杂的z与x1,x2,...x9的关系式

86
00:03:36,666 --> 00:03:39,199
 这就是一个用函数表达现实问题的例子

87
00:03:39,400 --> 00:03:41,433
 用于对一个图像做类别判断

88
00:03:41,700 --> 00:03:44,266
 我们前面说过, 不论是文字, 图像

89
00:03:44,266 --> 00:03:44,999
 音频

90
00:03:45,233 --> 00:03:47,199
 这些都可以被转化为数据存储

91
00:03:47,400 --> 00:03:48,866
 都可以作为函数参数

92
00:03:49,466 --> 00:03:51,066
 也就可以根据我们的目标输出

93
00:03:51,066 --> 00:03:52,133
 分析出一套规律

94
00:03:52,133 --> 00:03:53,099
 一个关系式

95
00:03:53,500 --> 00:03:56,433
对于先前这个问题我们是知道结果了

96
00:03:56,500 --> 00:03:57,933
 当然可以直接写出来

97
00:03:57,933 --> 00:04:00,833
 画出来, 但是现实中的问题很复杂

98
00:04:01,033 --> 00:04:03,033
 我们难以人工算出结果

99
00:04:03,166 --> 00:04:06,333
因此需要设计一个相对通用的神经网络模型结构

100
00:04:06,600 --> 00:04:09,466
 通过训练调整参数得到这个未知的结果 

101
00:04:10,100 --> 00:04:12,700
我们可以将y1,y2两个函数画成网状结构

102
00:04:12,700 --> 00:04:13,700
 形成了两层

103
00:04:14,266 --> 00:04:16,799
第一层是输入, 是三乘三的图像矩阵

104
00:04:17,233 --> 00:04:18,433
第二层是处理

105
00:04:18,466 --> 00:04:20,299
 在神经网络中称作隐藏层

106
00:04:20,666 --> 00:04:23,099
 实际上隐藏层也可以有多层递进的结构

107
00:04:23,300 --> 00:04:25,233
 来更好地拟合更复杂的情况

108
00:04:25,400 --> 00:04:27,000
 并不是只能是这样的一列

109
00:04:27,200 --> 00:04:30,433
但我们现在的问题只用到了一列
(单层隐藏层理论上就能拟合任意函数,  但是多层的表现更好)

110
00:04:30,433 --> 00:04:31,333
这里一个个单元

111
00:04:31,333 --> 00:04:32,933
 就是神经网络中的神经元

112
00:04:33,333 --> 00:04:34,399
神经元间的连线

113
00:04:34,400 --> 00:04:36,066
 就是先前提到的系数矩阵

114
00:04:36,100 --> 00:04:37,233
 这里叫做权重

115
00:04:38,000 --> 00:04:39,933
这里缺失的第三层, 就是输出

116
00:04:40,266 --> 00:04:42,666
我们要将隐藏层的结果汇总到输出层

117
00:04:42,833 --> 00:04:45,533
 最终输出我们希望神经网络预测的目标

118
00:04:46,600 --> 00:04:49,033
在这里, 就是刚才提到的z这个函数

119
00:04:49,900 --> 00:04:51,566
当然, 按照目前我们的画法

120
00:04:51,633 --> 00:04:52,533
 你会发现

121
00:04:52,600 --> 00:04:57,733
 这个"神经网络"只能表达y=ax1+bx2+cx3这样的形式

122
00:04:58,266 --> 00:05:00,699
有一个参数时, 是二维空间中的直线

123
00:05:01,033 --> 00:05:03,399
 两个参数时, 是三维空间中的平面

124
00:05:03,666 --> 00:05:05,833
 总之, 是线性的, 是"直"的

125
00:05:06,366 --> 00:05:08,566
而先前我们要得到的z的表达式

126
00:05:08,666 --> 00:05:10,766
 画出来是一个三维空间中的曲面

127
00:05:11,333 --> 00:05:13,866
 我们无法通过现在这个网络表达出曲面

128
00:05:14,066 --> 00:05:15,200
但我们可以转换思路

129
00:05:15,200 --> 00:05:17,866
 将我们的目标分解成一个个"直"的片段

130
00:05:18,100 --> 00:05:20,600
 然后组合成我们需要的曲线, 曲面

131
00:05:20,600 --> 00:05:23,000
 或是更高维度空间的其他不"直"的目标

132
00:05:23,700 --> 00:05:24,533
要达成这个目的

133
00:05:24,533 --> 00:05:26,133
 我们需要使用激活函数

134
00:05:26,600 --> 00:05:27,733
 例如常见的激活函数

135
00:05:27,733 --> 00:05:30,300
 sigmoid, 数学表达式是这样的

136
00:05:30,833 --> 00:05:32,333
其形状是这样的

137
00:05:33,300 --> 00:05:36,833
 sigmoid这类激活函数使原本线性的函数变得非线性

138
00:05:37,033 --> 00:05:38,500
最终我们可以对各种输入

139
00:05:38,500 --> 00:05:42,266
 通过简单的先前神经网络中能用的加法和系数乘法

140
00:05:42,266 --> 00:05:44,800
 组合片段, 拟合成任意连续的曲线

141
00:05:44,800 --> 00:05:46,200
 表达出任意的函数

142
00:05:46,666 --> 00:05:48,266
只要你的输入与输出目标

143
00:05:48,266 --> 00:05:50,900
 存在着逻辑上函数表达推理的可能性

144
00:05:51,100 --> 00:05:52,566
 能够写成函数形式

145
00:05:52,800 --> 00:05:55,233
 我们就实现了对具体问题的数学式表达

146
00:05:56,533 --> 00:05:57,266
目前为止

147
00:05:57,266 --> 00:06:00,500
 我们知道了实际问题可以转变为一种固定套路的数学式

148
00:06:00,733 --> 00:06:02,833
 可以用这种神经网络结构去推断

149
00:06:03,166 --> 00:06:04,800
 这个过程叫正向传播

150
00:06:05,000 --> 00:06:07,666
但是, 先前我们的数字判别问题中

151
00:06:07,666 --> 00:06:09,500
 各个公式是人工计算的

152
00:06:09,500 --> 00:06:11,600
如果先前计算式中的各个常量

153
00:06:11,700 --> 00:06:14,733
 也就是神经网络中的各个神经元间的连接参数(权重),

154
00:06:14,733 --> 00:06:16,333
 我们能让计算机运算出来

155
00:06:16,333 --> 00:06:18,966
就能实现对特定问题自动推理的人工智能

156
00:06:20,000 --> 00:06:22,066
我们可以先让各个权重随机生成

157
00:06:22,766 --> 00:06:24,166
 然后进行简单的测试

158
00:06:24,166 --> 00:06:25,500
 看传入输入的数据

159
00:06:25,600 --> 00:06:27,600
 输出结果是否是我们想要的

160
00:06:28,200 --> 00:06:29,966
 这个结果叫做预测结果(prediction),

161
00:06:29,966 --> 00:06:31,133
缩写为pred

162
00:06:31,933 --> 00:06:33,266
如果不是我们想要的

163
00:06:33,366 --> 00:06:35,633
 我们可以看看与我们想要的结果相差多少

164
00:06:36,166 --> 00:06:37,966
 我们想要的结果是真实结果

165
00:06:38,233 --> 00:06:39,966
 写作true(真),或target(目标),

166
00:06:39,966 --> 00:06:43,633
 这里就可以用到譬如均方误差(mean squared error, 简称mse)来度量与目标的差异

167
00:06:44,233 --> 00:06:47,200
其中这个符号读作sigma, 表示求和

168
00:06:47,333 --> 00:06:48,466
 从1加到n

169
00:06:49,100 --> 00:06:51,000
因为我们会有一大堆输入输出

170
00:06:51,033 --> 00:06:52,466
 要衡量总体的差异

171
00:06:53,033 --> 00:06:54,500
这个计算也很好理解

172
00:06:54,666 --> 00:06:56,666
 我们要衡量差异自然想到相减

173
00:06:56,933 --> 00:06:58,633
 而差异是没有正负的

174
00:06:58,733 --> 00:07:00,666
 所以我们通过平方消除负号

175
00:07:00,733 --> 00:07:02,933
 然后求和取平均得到误差

176
00:07:03,166 --> 00:07:04,500
采用平方而非绝对值

177
00:07:04,500 --> 00:07:05,900
 主要是为了方便求导

178
00:07:06,066 --> 00:07:09,033
 并且如果误差较大(大于1)平方还能放大差异

179
00:07:09,033 --> 00:07:12,100
 加大调整力度, 关于求导是什么

180
00:07:12,100 --> 00:07:13,000
 为什么求导

181
00:07:13,266 --> 00:07:15,633
 权重如何调整后面会进行讲解

182
00:07:16,833 --> 00:07:18,600
总之, 这种度量差异的函数

183
00:07:18,733 --> 00:07:20,300
 我们称之为损失函数(loss function),

184
00:07:20,833 --> 00:07:22,533
表示当前效果和目标相比

185
00:07:22,533 --> 00:07:24,966
 有多大风险, 多大损失(loss)

186
00:07:24,966 --> 00:07:26,566
我们希望它的值尽可能小

187
00:07:26,766 --> 00:07:28,433
 意味着尽可能接近目标

188
00:07:28,966 --> 00:07:30,566
譬如, 是/否

189
00:07:30,766 --> 00:07:35,099
可以表达为1,0, 假如AI算出结果为0.9

190
00:07:35,300 --> 00:07:37,000
虽然不完全是我们的目标

191
00:07:37,133 --> 00:07:39,300
 但当结果只可能是两个的时候

192
00:07:39,333 --> 00:07:41,200
 0.9已经接近"是"了

193
00:07:41,466 --> 00:07:43,266
 损失函数结果会很小

194
00:07:43,300 --> 00:07:44,866
意味着我们接近了目标

195
00:07:45,100 --> 00:07:47,100
因此, 接下来, 我们要做的

196
00:07:47,100 --> 00:07:49,500
 就是对先前随机生成的权重调整

197
00:07:49,566 --> 00:07:52,466
 并计算损失函数, 去不断降低损失

198
00:07:53,433 --> 00:07:55,466
漫无目的的随机显然是不行的

199
00:07:55,600 --> 00:07:57,766
 许多现实问题的参数量十分庞大

200
00:07:57,833 --> 00:07:59,466
 我们需要一种可靠的方法

201
00:08:00,500 --> 00:08:02,833
通过偏导数入手是一种常见的方法

202
00:08:03,333 --> 00:08:04,633
 什么是偏导数呢

203
00:08:05,000 --> 00:08:06,666
 首先, 二维平面内的直线

204
00:08:06,666 --> 00:08:08,966
 我们知道有斜率来描述倾斜程度

205
00:08:09,333 --> 00:08:10,400
如果是曲线

206
00:08:10,466 --> 00:08:12,866
 那么曲线的不同位置斜率是不同的

207
00:08:13,266 --> 00:08:14,933
 我们通过对特定位置求导数

208
00:08:14,933 --> 00:08:16,000
 来计算斜率

209
00:08:16,466 --> 00:08:18,300
 导数就是特定位置的斜率

210
00:08:20,000 --> 00:08:22,200
 求导数有一系列数学方法/公式

211
00:08:22,200 --> 00:08:23,433
 但不是这里的重点

212
00:08:24,100 --> 00:08:25,666
而如果参数量增加

213
00:08:25,733 --> 00:08:27,166
 例如到了三维空间

214
00:08:27,466 --> 00:08:29,100
 曲面上的一点有很多切线

215
00:08:29,100 --> 00:08:30,633
 有很多不同情况下的"斜率",

216
00:08:30,833 --> 00:08:33,433
 我们固定一个面, 形成一个平面曲线

217
00:08:33,733 --> 00:08:36,000
 把原先其他的变量都看作常数

218
00:08:36,233 --> 00:08:38,133
 然后对此时唯一的参数求导

219
00:08:38,133 --> 00:08:39,133
 就是偏导数

220
00:08:39,433 --> 00:08:41,233
有了偏导数/斜率之后

221
00:08:41,333 --> 00:08:42,466
 可以做些什么呢

222
00:08:42,866 --> 00:08:43,833
神经网络中

223
00:08:43,866 --> 00:08:45,533
 我们将各个式子层层代入

224
00:08:45,533 --> 00:08:47,466
 可以得到一个预测结果和输入

225
00:08:47,466 --> 00:08:48,733
 及权重的关系式

226
00:08:49,433 --> 00:08:51,200
预测结果代入损失函数

227
00:08:51,266 --> 00:08:53,466
 又能得到损失函数与它们的关系式

228
00:08:53,733 --> 00:08:58,200
 我们可以将损失函数与权重的关系式简写为L(w1,w2,...),

229
00:08:58,200 --> 00:08:59,833
w是权重weight的缩写

230
00:09:00,533 --> 00:09:02,433
斜率的实际意义是变化速度

231
00:09:02,800 --> 00:09:06,100
 如果我们对损失函数求各权重变量的偏导数

232
00:09:06,266 --> 00:09:07,066
 就可以衡量

233
00:09:07,066 --> 00:09:10,033
 各个权重对损失函数的变化速度的影响

234
00:09:10,466 --> 00:09:13,233
如果一个权重变量对损失函数构成剧烈的影响

235
00:09:13,600 --> 00:09:14,800
 偏导数值很大

236
00:09:14,933 --> 00:09:18,700
 我们就知道这个权重的波动会轻松引起预测结果的波动

237
00:09:19,800 --> 00:09:20,900
例如一张图像

238
00:09:21,000 --> 00:09:24,433
 仅仅一个像素的颜色受权重波动影响变化

239
00:09:24,466 --> 00:09:27,233
 就导致了神经网络把一只狗预测成了猫(结果波动),

240
00:09:27,233 --> 00:09:28,533
 这显然是不合理的

241
00:09:29,100 --> 00:09:32,200
这个权重变量很大程度导致了神经网络的预测失败

242
00:09:32,266 --> 00:09:32,900
 也就是说

243
00:09:32,900 --> 00:09:35,200
 我们需要调整权重, 减小它的干涉

244
00:09:36,300 --> 00:09:38,533
调整的方式, 可以直接做简单的减法

245
00:09:38,766 --> 00:09:40,666
 比如权重是1, 导数是10

246
00:09:41,333 --> 00:09:44,200
直接1-0.01*10得到0.9

247
00:09:44,766 --> 00:09:46,400
通过图像观察我们能发现

248
00:09:46,400 --> 00:09:48,466
 损失函数值往低的方向走了

249
00:09:48,666 --> 00:09:51,100
其中0.01是我们微调权重的系数

250
00:09:51,500 --> 00:09:52,500
 称作学习率

251
00:09:52,666 --> 00:09:54,233
 避免直接大幅度调整曲线

252
00:09:54,266 --> 00:09:55,766
 导致结果大幅度波动

253
00:09:56,466 --> 00:09:58,000
另一种情况, 导数是负数

254
00:09:58,000 --> 00:10:03,400
 比如-10, 同样变化率很高, -1-0.01*(-10)=-0.9

255
00:10:03,533 --> 00:10:04,600
权重增加了

256
00:10:04,800 --> 00:10:06,600
 损失函数也往低的方向走了

257
00:10:07,033 --> 00:10:08,666
最终, 只有到达一个波谷

258
00:10:08,866 --> 00:10:10,433
 此时变化率是平缓的

259
00:10:10,800 --> 00:10:12,200
 权重不再被调整

260
00:10:12,433 --> 00:10:14,900
 损失函数也降到了低点

261
00:10:14,900 --> 00:10:15,233
现在

262
00:10:15,233 --> 00:10:17,400
 我们得到了具体的调整权重的方法

263
00:10:17,633 --> 00:10:20,033
 这个方法被称为随机梯度下降法

264
00:10:20,600 --> 00:10:22,466
 其中梯度与偏导数有关

265
00:10:22,500 --> 00:10:23,833
 也具有几何意义

266
00:10:23,833 --> 00:10:25,366
 但同样不是这里的重点

267
00:10:25,633 --> 00:10:27,166
 在这里它只是一个名字

268
00:10:27,466 --> 00:10:28,766
这个方法的作用下

269
00:10:28,800 --> 00:10:31,000
 最终损失函数的结果将趋于稳定

270
00:10:32,433 --> 00:10:34,666
先前预测的过程, 称作正向传播

271
00:10:35,066 --> 00:10:37,066
 而得到预测结果之后, 计算偏导

272
00:10:37,066 --> 00:10:39,600
 反过来去调整权重, 就称为反向传播

273
00:10:41,833 --> 00:10:42,866
有了这样两条回路

274
00:10:42,866 --> 00:10:46,166
 我们就可以不断给我们的网络提供各种预先准备好的数据

275
00:10:46,300 --> 00:10:48,966
 让他推理, 比较和预期结果的差异

276
00:10:49,033 --> 00:10:50,733
 然后改善自身的网络权重

277
00:10:51,333 --> 00:10:53,500
这样整个过程, 就是神经网络的训练

278
00:10:53,600 --> 00:10:54,166
 最终

279
00:10:54,166 --> 00:10:56,200
 我们得到了一个越发确定的权重

280
00:10:57,133 --> 00:10:59,366
当然最终这个权重不一定是最优解

281
00:10:59,366 --> 00:10:59,766
 例如

282
00:10:59,766 --> 00:11:02,100
 有一个损失函数与两个输入参数有关

283
00:11:02,233 --> 00:11:04,266
 他们的权重是w1,w2

284
00:11:04,266 --> 00:11:06,300
我们可以将其绘制成一个三维图形

285
00:11:06,733 --> 00:11:08,500
样子就是这样一个个小山坡

286
00:11:09,066 --> 00:11:11,533
 其中最低的点, 也就是损失最小的点

287
00:11:11,600 --> 00:11:14,166
 就是我们需要的w1,w2两个权重的值

288
00:11:14,533 --> 00:11:16,333
但是按照先前我们通过算偏导

289
00:11:16,333 --> 00:11:17,500
 随机梯度下降

290
00:11:17,500 --> 00:11:19,733
 让损失函数趋于稳定的方法, 意味着

291
00:11:19,733 --> 00:11:21,133
 我们最终找到的权重

292
00:11:21,133 --> 00:11:23,033
 可能是我们预期的山坡底部

293
00:11:23,133 --> 00:11:24,766
 也有可能是平缓的坡顶

294
00:11:25,100 --> 00:11:26,933
 或是并不最接近0的坡底

295
00:11:26,966 --> 00:11:28,833
这些地方同样变化率平缓

296
00:11:28,933 --> 00:11:30,200
 权重难以调整

297
00:11:30,833 --> 00:11:33,000
 这表示, 我们找到的是局部最优解

298
00:11:33,566 --> 00:11:35,366
如何尽可能找到理论最优解

299
00:11:35,366 --> 00:11:37,466
 是人工智能领域的一个常见问题

300
00:11:37,766 --> 00:11:38,866
 譬如引入冲量

301
00:11:38,866 --> 00:11:42,033
 根据先前的变化量对这次的变化加速或减速

302
00:11:42,066 --> 00:11:43,366
 让我们在拟合的过程中

303
00:11:43,366 --> 00:11:45,066
 不再停留于局部最优处

304
00:11:45,166 --> 00:11:47,033
 而是冲过一些平缓的小坡

305
00:11:47,100 --> 00:11:48,566
 冲过这些局部最优点

306
00:11:48,733 --> 00:11:50,900
 最终提高到达理论最优点的可能性

307
00:11:50,900 --> 00:11:53,966
总而言之, 激活函数, 损失函数

308
00:11:53,966 --> 00:11:55,933
 反向传播优化(如刚才提到的梯度下降法调整权重)等等

309
00:11:56,566 --> 00:11:57,633
 有各种各样的方法

310
00:11:57,633 --> 00:12:00,300
 这些都是深度学习理论的研究要点

311
00:12:00,300 --> 00:12:01,233
以上所有的知识

312
00:12:01,233 --> 00:12:03,600
 已经足够编写一个完整的神经网络了

313
00:12:03,600 --> 00:12:04,200
接下来

314
00:12:04,200 --> 00:12:07,433
 我将讲解我使用这些知识编写的一个简易神经网络

315
00:12:07,433 --> 00:12:09,800
 其在github上的开源链接是这个

316
00:12:10,033 --> 00:12:12,600
 你也可以从简介或者评论区置顶部分找到

317
00:12:12,633 --> 00:12:14,500
训练和推断的部分

318
00:12:14,533 --> 00:12:16,366
 在nn.py这个文件中

319
00:12:16,833 --> 00:12:18,466
 nn就是神经网络(neural network)的缩写

320
00:12:18,600 --> 00:12:19,666
其核心, 是从这里

321
00:12:19,666 --> 00:12:21,966
 到这里, 一共约100行代码

322
00:12:22,533 --> 00:12:25,566
首先是初始化, 我们需要提供输入量

323
00:12:25,800 --> 00:12:27,500
 比如一副图像的像素数量

324
00:12:27,700 --> 00:12:29,900
 然后是隐藏层中神经元的数量

325
00:12:30,400 --> 00:12:31,666
 接着是激活函数

326
00:12:32,300 --> 00:12:34,300
 默认使用刚才介绍过的sigmoid

327
00:12:34,466 --> 00:12:37,033
注意每个神经元可以使用独立的激活函数

328
00:12:37,166 --> 00:12:39,533
 这里我是偷懒了全部使用了同一个

329
00:12:39,600 --> 00:12:41,000
接着, 有了初始化参数

330
00:12:41,000 --> 00:12:42,733
 我们就确定了神经网络的结构

331
00:12:43,100 --> 00:12:43,866
 在这里

332
00:12:43,866 --> 00:12:46,233
 对各个权重和偏移量随机初始化

333
00:12:46,633 --> 00:12:50,500
 偏移量是权重系数和输入相乘累加后再额外添加的一个数值

334
00:12:50,500 --> 00:12:51,500
 但不是必须的

335
00:12:51,500 --> 00:12:53,500
是为了能够对我们的函数图像平移

336
00:12:53,500 --> 00:12:54,633
使其更加灵活

337
00:12:54,800 --> 00:12:56,366
之后是正向传播

338
00:12:56,533 --> 00:12:58,966
 hidden_layers就是各个神经元的输出

339
00:12:59,033 --> 00:13:01,966
 它的结果是, 前面各个权重连线累加

340
00:13:01,966 --> 00:13:03,566
 然后使用激活函数处理

341
00:13:03,733 --> 00:13:05,500
注意这里我只用了单层隐藏层

342
00:13:05,500 --> 00:13:06,466
 而不是多层

343
00:13:06,900 --> 00:13:08,533
 所以已经算完了隐藏层

344
00:13:08,533 --> 00:13:08,833
最终

345
00:13:08,833 --> 00:13:11,400
 我们将隐藏层同样累加汇总到输出

346
00:13:11,400 --> 00:13:13,866
 得到了一个推断结果

347
00:13:13,866 --> 00:13:15,866
后一个函数是训练与反向传播的部分

348
00:13:16,200 --> 00:13:18,100
第一个大循环, 就是一轮一轮的训练

349
00:13:18,766 --> 00:13:21,166
 我们要在一轮轮训练中不断对权重微调

350
00:13:22,333 --> 00:13:23,233
第二个循环

351
00:13:23,233 --> 00:13:24,766
 就是对每一轮训练中的数据

352
00:13:24,766 --> 00:13:27,066
 进行推断, 比较和预期结果的差异

353
00:13:27,233 --> 00:13:28,533
其中, data这个变量

354
00:13:28,533 --> 00:13:30,099
就是我们训练用到的数据

355
00:13:30,666 --> 00:13:33,033
而旁边这个变量就是我们预期的推断目标 

356
00:13:33,566 --> 00:13:35,499
这里d_L_d_pred中的d

357
00:13:35,500 --> 00:13:36,733
指的是微分(不知道微分没关系),

358
00:13:36,733 --> 00:13:38,766
 L是loss的缩写, 即损失函数

359
00:13:38,766 --> 00:13:40,666
 pred是prediction的缩写

360
00:13:40,666 --> 00:13:41,400
 即预测/推断

361
00:13:41,400 --> 00:13:46,000
 d_L_d_pred也就是损失函数对预测结果的导数/变化率

362
00:13:46,400 --> 00:13:48,200
这里没有直接计算损失函数

363
00:13:49,100 --> 00:13:51,900
 这里可以看到损失函数mse_loss

364
00:13:51,900 --> 00:13:54,000
是对一系列目标和预测结果相减

365
00:13:54,000 --> 00:13:55,666
 求平方, 然后取均值

366
00:13:56,333 --> 00:13:58,600
 就是我们先前已经提到过的均方误差

367
00:13:58,800 --> 00:14:00,133
而之前我们提到过

368
00:14:00,133 --> 00:14:01,600
 我们要计算损失函数

369
00:14:01,600 --> 00:14:02,433
 取导数

370
00:14:02,533 --> 00:14:05,266
 然后去调整权重, 使损失趋于稳定

371
00:14:05,600 --> 00:14:06,100
 因此

372
00:14:06,100 --> 00:14:08,333
 损失函数就是最终函数式的一份子

373
00:14:08,700 --> 00:14:09,866
 因此在这个程序里

374
00:14:10,100 --> 00:14:12,133
 我们直接计算了均方误差的导数

375
00:14:12,266 --> 00:14:18,500
 这里直接使用高中求导知识就可以得到结果是
-2 * (output_target - output_prediction)

376
00:14:18,500 --> 00:14:20,900
接下来一系列d_x_d_y形式的变量

377
00:14:20,900 --> 00:14:23,166
 都是对神经网络各个连接部分在进行求导

378
00:14:23,333 --> 00:14:24,266
而这里的一些缩写

379
00:14:24,533 --> 00:14:27,333
 比如w指的就是权重weight, 而比如hl

380
00:14:27,333 --> 00:14:29,366
指的就是隐藏层hidden layer

381
00:14:30,000 --> 00:14:32,000
具体的公式方法这里不作讲解

382
00:14:32,466 --> 00:14:34,400
 程序中用到的都是高中的知识

383
00:14:34,700 --> 00:14:36,366
最后, 就是对权重的调整

384
00:14:36,533 --> 00:14:38,433
 包括这里的学习率learn_rate在内

385
00:14:38,533 --> 00:14:39,633
 先前已经讲过了

386
00:14:39,800 --> 00:14:41,466
最后再次计算的损失函数值

387
00:14:41,466 --> 00:14:43,000
 只是用于显示到屏幕上

388
00:14:43,000 --> 00:14:44,866
 给训练人员查看

389
00:14:44,866 --> 00:14:47,333
这就是整个简单的神经网络的程序逻辑了

390
00:14:48,233 --> 00:14:50,666
main.py内是具体的简单使用示例

391
00:14:50,900 --> 00:14:52,600
 这里用四个3x3矩阵

392
00:14:52,700 --> 00:14:55,333
 形状分别是0, 1, 4, 7

393
00:14:55,466 --> 00:14:57,633
就是刚才我们的例子, 作为输入

394
00:14:58,400 --> 00:14:59,666
然后设置他们的预期目标

395
00:14:59,766 --> 00:15:01,099
即0, 1, 4, 7

396
00:15:01,400 --> 00:15:03,233
这里还对目标全部除以了10

397
00:15:03,333 --> 00:15:05,366
是为了让我们的所有数据归一化

398
00:15:05,400 --> 00:15:06,966
 分布在0-1之间

399
00:15:07,300 --> 00:15:10,100
 因为我们的权重会初始化在很小的数值范围内

400
00:15:10,300 --> 00:15:12,300
调整权重时的量也是很小的微调

401
00:15:12,733 --> 00:15:14,100
 如果我们的输入很大

402
00:15:14,133 --> 00:15:16,400
 经过激活函数后, 数值依然较大

403
00:15:16,666 --> 00:15:17,966
 最终预测结果巨大

404
00:15:17,966 --> 00:15:19,433
 求导依然是大数值

405
00:15:19,566 --> 00:15:21,600
 进而导致权重被大幅度更新

406
00:15:21,733 --> 00:15:22,633
 剧烈波动

407
00:15:23,433 --> 00:15:26,300
如果权重一开始就随机的很大或是很小的负数

408
00:15:26,400 --> 00:15:27,733
 也会面临同样的问题

409
00:15:27,966 --> 00:15:29,500
 这种问题叫做梯度爆炸

410
00:15:29,600 --> 00:15:31,966
归一化可以让我们的模型更好地拟合目标

411
00:15:31,966 --> 00:15:33,300
 并且避免梯度爆炸

412
00:15:34,300 --> 00:15:35,000
总之, 最后

413
00:15:35,000 --> 00:15:36,866
 我们创建先前写好的神经网络

414
00:15:36,866 --> 00:15:38,333
 输入是3x3的矩阵

415
00:15:38,333 --> 00:15:39,833
 隐藏层有9个神经元

416
00:15:39,933 --> 00:15:42,366
 然后训练, 传入数据和目标

417
00:15:43,266 --> 00:15:44,866
训练完毕后, 再进行预测

418
00:15:45,066 --> 00:15:47,533
 预测的实现就是调用了刚才的正向传播

419
00:15:48,866 --> 00:15:50,333
 可以看到预测成功了

420
00:15:50,500 --> 00:15:52,733
 你也可以自己调整隐藏层神经元的数量

421
00:15:52,733 --> 00:15:54,133
 再看看预测的成功率

422
00:15:54,800 --> 00:15:56,000
当然, 这里数据很少

423
00:15:56,000 --> 00:15:57,433
 会出现过拟合问题

424
00:15:57,600 --> 00:16:00,933
 也就是这个神经网络会变得只为了这个数据和结果而生

425
00:16:01,400 --> 00:16:03,100
并且, 这里数据既用于训练

426
00:16:03,100 --> 00:16:05,066
 又用于测试, 是不合适的

427
00:16:05,133 --> 00:16:06,000
 就算全部成功

428
00:16:06,000 --> 00:16:08,166
 我们也不知道对其他数据的效果如何

429
00:16:09,500 --> 00:16:11,200
接下来, 则是进阶部分了

430
00:16:11,733 --> 00:16:13,800
 如何对人手写的数字进行识别?

431
00:16:14,700 --> 00:16:16,300
如果你直接用这个网络训练

432
00:16:16,300 --> 00:16:18,966
 试图让它一口气能够判断0-9你会发现

433
00:16:18,966 --> 00:16:21,133
 最后神经网络总是输出一个数字

434
00:16:21,133 --> 00:16:22,533
这是因为, 无脑乱猜

435
00:16:22,533 --> 00:16:24,133
 或者只输出同一个数字

436
00:16:24,166 --> 00:16:25,500
 最后成功率都是10%,

437
00:16:26,300 --> 00:16:28,300
最终的均方误差是趋于稳定的

438
00:16:28,300 --> 00:16:29,900
 损失函数是稳定的

439
00:16:30,333 --> 00:16:31,733
 符合我们的设计目标

440
00:16:31,733 --> 00:16:34,133
也就是AI摆烂了, 在我们的规则内

441
00:16:34,133 --> 00:16:35,233
 拿到保底走人

442
00:16:35,466 --> 00:16:36,566
实际上, 手写的数字

443
00:16:36,566 --> 00:16:39,733
 情况非常复杂, 例如数字1和7很相似

444
00:16:39,933 --> 00:16:42,833
 3和5很相似, 6和9很相似

445
00:16:43,166 --> 00:16:45,033
 有时候人类都不一定能辨别

446
00:16:45,266 --> 00:16:46,066
 对ai来说

447
00:16:46,066 --> 00:16:48,900
 输出0.1和0.7, 这之间差距太大了

448
00:16:49,033 --> 00:16:50,600
 需要的权重差异很大

449
00:16:50,800 --> 00:16:53,600
 但实际上这两种情况对我们来说却是相似的

450
00:16:53,733 --> 00:16:54,966
 这可太难办了

451
00:16:55,900 --> 00:16:57,533
因此, 我们需要转换思路

452
00:16:57,833 --> 00:16:59,533
 比如, 可以用十个神经网络

453
00:16:59,700 --> 00:17:01,633
 每个网络只对一个数字判断

454
00:17:01,900 --> 00:17:03,866
 例如只判断这个数字是不是5

455
00:17:03,900 --> 00:17:05,100
要么是要么否

456
00:17:05,400 --> 00:17:06,666
我们手写一个数字

457
00:17:06,800 --> 00:17:08,100
 十个网络一起工作

458
00:17:08,100 --> 00:17:08,600
 最后

459
00:17:08,600 --> 00:17:12,033
 看哪个网络判断是的概率最大(结果最接近1),

460
00:17:12,300 --> 00:17:14,666
我们就视作神经网络判断的是这个数字

461
00:17:14,666 --> 00:17:17,066
在usps文件夹里

462
00:17:17,066 --> 00:17:18,433
 train.py这个文件中

463
00:17:18,433 --> 00:17:21,833
 我用这种方法对usps的手写数据进行了训练

464
00:17:21,833 --> 00:17:24,000
 并且仅使用了单神经元的隐藏层

465
00:17:24,533 --> 00:17:26,600
 最终测试准确率接近90%

466
00:17:27,166 --> 00:17:28,866
我对权重做了一些简化

467
00:17:29,033 --> 00:17:30,266
 只保留整数部分

468
00:17:30,266 --> 00:17:31,933
 准确率依然有87%,

469
00:17:32,400 --> 00:17:34,866
 深度学习领域也有许多对模型裁切的研究

470
00:17:34,866 --> 00:17:37,100
 我这种简化方式是十分粗暴的

471
00:17:37,300 --> 00:17:40,200
将权重保存为json文件后, 可以看到

472
00:17:40,266 --> 00:17:41,966
 这里只有这么一丁点的数字

473
00:17:42,166 --> 00:17:44,966
 10个网络权重压缩后, 体积仅约1kb

474
00:17:44,966 --> 00:17:46,433
连一张常规图片都不到

475
00:17:46,433 --> 00:17:49,433
 这就实现了非常粗略的手写数字识别

476
00:17:49,433 --> 00:17:51,100
当然, 这个神经网络非常简单

477
00:17:51,133 --> 00:17:52,233
隐藏层仅有一层

478
00:17:52,233 --> 00:17:54,766
 并且还有很多深度学习领域的方法没有应用

479
00:17:55,133 --> 00:17:57,033
例如多批量输入的快速训练

480
00:17:57,066 --> 00:17:59,066
对图像处理更高效的卷积操作

481
00:17:59,466 --> 00:18:01,133
压缩特征的池化操作

482
00:18:01,200 --> 00:18:05,566
 对分类问题更有效的softmax激活函数和交叉熵损失函数

483
00:18:06,100 --> 00:18:10,133
常见的神经网络对于这种数字检测能够达到99%以上的准确率

484
00:18:10,333 --> 00:18:12,900
感兴趣的也可以自己根据关键词进行搜索

485
00:18:12,933 --> 00:18:13,800
除了分类之外

486
00:18:13,800 --> 00:18:16,700
 输出也可以是比如一个x轴或y轴坐标

487
00:18:16,766 --> 00:18:19,066
 一个物体形体的宽度, 高度

488
00:18:19,300 --> 00:18:20,466
 这些结合起来

489
00:18:20,466 --> 00:18:21,700
 就成了目标检测

490
00:18:21,933 --> 00:18:24,066
 神经网络能做到的, 远不止如此

491
00:18:24,500 --> 00:18:26,300
不过, 神经网络也不是万能的

492
00:18:26,533 --> 00:18:28,833
不是尽可能复杂就能拟合一切问题

493
00:18:29,266 --> 00:18:31,133
神经网络能拟合的, 只是函数

494
00:18:31,300 --> 00:18:33,066
 设定的输出也不是任选

495
00:18:33,300 --> 00:18:35,100
而是需要逻辑层面的可能性

496
00:18:36,033 --> 00:18:37,166
相信看到这里的大家

497
00:18:37,166 --> 00:18:38,766
 对神经网络的工作原理, 细节

498
00:18:38,766 --> 00:18:40,566
已经有了相当具体的认识

499
00:18:41,466 --> 00:18:45,266
神经网络其实就像是根据你的数据和目标进行规律总结

500
00:18:45,766 --> 00:18:47,166
 你要设计合适的网络模型

501
00:18:47,166 --> 00:18:48,233
 运用各种技巧

502
00:18:48,233 --> 00:18:50,033
 尽可能找到这个预期的规律

503
00:18:50,033 --> 00:18:54,966
假如你设想的输入输出存在逻辑上可能的数学表达关系 理论上就能拟合

504
00:18:56,133 --> 00:18:57,300
假如能够明白大脑

505
00:18:57,300 --> 00:19:04,266
 各种细胞的工作原理 将每个细胞 原子 数字化 将整个地球数字化 也许就可以预知未来

506
00:19:05,500 --> 00:19:07,233
最后, 关于AI绘画

507
00:19:08,033 --> 00:19:09,433
 ai是在总结规律

508
00:19:09,466 --> 00:19:13,400
本身并没有思想，ai绘画不会进行通俗理解的抄袭缝合

509
00:19:13,800 --> 00:19:15,766
但可能存在规律套路上的缝合

510
00:19:16,600 --> 00:19:18,633
如果要用一种人类好理解的方式

511
00:19:18,700 --> 00:19:19,500
 类似于

512
00:19:19,600 --> 00:19:22,066
 传入的标签代表的大概是什么样子的

513
00:19:22,400 --> 00:19:23,666
 这个腿部是这样的

514
00:19:23,666 --> 00:19:26,766
 接下来的部分质感的规律应该这么画或者那么画

515
00:19:26,866 --> 00:19:27,766
 而这个规律

516
00:19:27,766 --> 00:19:30,266
 确实源于AI用作数据的画师的作品

517
00:19:30,400 --> 00:19:32,100
 但由AI分析训练得到

518
00:19:32,466 --> 00:19:33,066
此外

519
00:19:33,066 --> 00:19:35,266
 如果你给定的数据和目标就是抄袭

520
00:19:35,366 --> 00:19:36,933
那么ai也会一并照办

521
00:19:37,100 --> 00:19:39,166
因此你可能看到ai直接连同构图

522
00:19:39,166 --> 00:19:40,700
画风一并模仿的情况

523
00:19:41,566 --> 00:19:42,466
对于未来

524
00:19:42,600 --> 00:19:44,700
 AI训练出一个初步的成果之后

525
00:19:44,700 --> 00:19:47,100
 可以根据人类的偏好, 进行定向调整

526
00:19:47,133 --> 00:19:48,266
 设定进化方向

527
00:19:48,500 --> 00:19:50,733
 例如这个画大众觉得怎样更好看

528
00:19:50,966 --> 00:19:53,166
 开发人员可以相应调整训练数据

529
00:19:53,300 --> 00:19:55,366
 保留讨喜的数据, 进一步深化

530
00:19:55,800 --> 00:19:57,733
 也许能够逐渐形成自己的风格

531
00:19:57,966 --> 00:20:00,133
 即总结出了大众喜好的规律

532
00:20:00,833 --> 00:20:04,699
 完全体, 相当于是成千上万的人对着画不断挑刺调整

533
00:20:04,700 --> 00:20:05,600
 得到的结果

534
00:20:05,600 --> 00:20:08,066
 我想必然会对相关行业造成巨大冲击

535
00:20:08,266 --> 00:20:11,466
但我也相信，ai能为人类带来更多的可能

536
00:20:11,633 --> 00:20:12,900
毕竟，科技的发展

537
00:20:12,900 --> 00:20:14,900
确实为人类带来了更好的生活

538
00:20:15,066 --> 00:20:16,733
而人类永无止尽的好奇心

539
00:20:16,733 --> 00:20:18,633
 对未知事物的理解和尝试

540
00:20:18,633 --> 00:20:20,300
 是AI始终无法替代的

541
00:20:21,100 --> 00:20:23,766
 人类最终要做的事, 只有人类自己知道

